{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU",
    "gpuClass": "standard"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/kang9366/English-Pronunciation-Classification/blob/master/English_Pronunciation_Classification.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "id": "1Orvat767_Id"
      },
      "outputs": [],
      "source": [
        "import numpy as np\n",
        "import tensorflow as tf\n",
        "from tensorflow import keras\n",
        "import matplotlib.pyplot as plt\n",
        "from sklearn.model_selection import train_test_split\n",
        "from tensorflow.keras.optimizers import SGD, Adam, RMSprop, Adadelta\n",
        "from tensorflow.keras.utils import to_categorical\n",
        "from tensorflow.keras.models import Sequential\n",
        "from tensorflow.keras.layers import Dense, Dropout, Flatten, Activation, MaxPool2D, BatchNormalization, Conv2D, MaxPooling2D\n",
        "from tensorflow.keras.preprocessing.image import ImageDataGenerator\n",
        "from tensorflow.keras.models import Model\n",
        "from tensorflow.keras.layers import Input, Convolution2D, BatchNormalization, Flatten, Dropout, Dense, AveragePooling2D,MaxPooling2D, Add, GlobalAveragePooling2D\n",
        "from tensorflow.keras.callbacks import EarlyStopping,ModelCheckpoint\n",
        "from sklearn.model_selection import KFold\n",
        "from sklearn.metrics import accuracy_score\n",
        "\n",
        "import pandas as pd\n",
        "import numpy as np\n",
        "import os\n",
        "import shutil\n",
        "from tqdm import tqdm\n",
        "from glob import glob\n",
        "import tensorflow\n",
        "import librosa\n",
        "import warnings\n",
        "import gc\n",
        "warnings.filterwarnings(\"ignore\")"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from google.colab import drive\n",
        "drive.mount('/content/drive')"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "2QetPOk38rFq",
        "outputId": "9fe50c10-8d47-47d7-a8fa-67bfbe1712c4"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Drive already mounted at /content/drive; to attempt to forcibly remount, call drive.mount(\"/content/drive\", force_remount=True).\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "cd /content/drive/MyDrive"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "rMjTEpP8-VcZ",
        "outputId": "40782ec8-a816-46ad-eef8-61183896a33d"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "/content/drive/MyDrive\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "def load_data(paths):\n",
        "    result = []\n",
        "    for path in tqdm(paths):\n",
        "        data, sr = librosa.load(path, sr = 16000)\n",
        "        result.append(data)\n",
        "    result = np.array(result)\n",
        "    return result"
      ],
      "metadata": {
        "id": "mEQFJoEG-Oji"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "main_path = 'Data'"
      ],
      "metadata": {
        "id": "VSg8pgOvmx1m"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "africa_train_data = np.load(main_path +\"/npy_data/africa_npy.npy\", allow_pickle = True)\n",
        "australia_train_data = np.load(main_path +\"/npy_data/australia_npy.npy\", allow_pickle = True)\n",
        "canada_train_data = np.load(main_path +\"/npy_data/canada_npy.npy\", allow_pickle = True)\n",
        "england_train_data = np.load(main_path +\"/npy_data/england_npy.npy\", allow_pickle = True)\n",
        "hongkong_train_data = np.load(main_path +\"/npy_data/hongkong_npy.npy\", allow_pickle = True)\n",
        "us_train_data = np.load(main_path +\"/npy_data/us_npy.npy\", allow_pickle = True)\n",
        "test_data = np.load(main_path +\"/npy_data/test_npy.npy\", allow_pickle = True)\n",
        "\n",
        "print(len(africa_train_data))\n",
        "print(len(us_train_data))\n",
        "print(len(hongkong_train_data))\n",
        "print(len(england_train_data))\n",
        "print(len(canada_train_data))\n",
        "print(len(australia_train_data))\n",
        "\n",
        "print(len(test_data))\n",
        "train_data_list = [africa_train_data, australia_train_data, canada_train_data, england_train_data, hongkong_train_data, us_train_data]"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "UpT0hcyy-n5A",
        "outputId": "dea2e449-a836-4a25-b538-9e50d3e9e102"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "1000\n",
            "1000\n",
            "1000\n",
            "1000\n",
            "1000\n",
            "1000\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [],
      "metadata": {
        "id": "M0MQ4dYpLwdU"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "print(len(test_data))"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "YHipFLF6mki7",
        "outputId": "43fea185-d43e-48c9-be16-a3da60024144"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "6100\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "X_train = np.concatenate(train_data_list, axis= 0)"
      ],
      "metadata": {
        "id": "vTMX25KO-xfT"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "X_test = np.array(test_data)"
      ],
      "metadata": {
        "id": "4BHMHvzh_REY"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# train_data의 label을 생성해 줍니다.\n",
        "y_train = np.concatenate((np.zeros(1000, dtype = np.int),\n",
        "                          np.ones(1000, dtype = np.int),\n",
        "                          np.ones(1000, dtype = np.int) * 2,\n",
        "                          np.ones(1000, dtype = np.int) * 3,\n",
        "                          np.ones(1000, dtype = np.int) * 4,\n",
        "                          np.ones(1000, dtype = np.int) * 5), axis = 0)\n",
        "y_train = to_categorical(y_train)"
      ],
      "metadata": {
        "id": "-msvcwQX_S_r"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "print(\"X_train shape : \", X_train.shape)\n",
        "print(\"Train size : \", X_train.shape[0])\n",
        "print(\"Validation size : \", X_val.shape[0])\n",
        "print(\"Test size : \", X_test.shape[0])"
      ],
      "metadata": {
        "id": "7fBfWgEXEwQl"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def delete(list):\n",
        "  for i in list:\n",
        "    del i\n",
        "  gc.collect()"
      ],
      "metadata": {
        "id": "7O1GPfheoe0Y"
      },
      "execution_count": 2,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "delete([africa_train_data, australia_train_data, canada_train_data , england_train_data, hongkong_train_data, us_train_data, test_data, train_data_list])"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "_WDw6HIX_Vr8",
        "outputId": "cf692d2a-417c-4d2b-9dfb-8d3cb94e6525"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "45"
            ]
          },
          "metadata": {},
          "execution_count": 27
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Normalization / Standardization "
      ],
      "metadata": {
        "id": "vmJglZujBsVn"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def minmax_scaling(data, sr = 16000, n_fft = 1024, win_length = 1024, hop_length = 160, n_mels = 64):\n",
        "    mel = []\n",
        "    for i in tqdm(data):\n",
        "        mel_ = librosa.feature.melspectrogram(i[:80000], sr = sr, n_fft = n_fft, win_length = win_length, hop_length = hop_length, n_mels = n_mels)\n",
        "        mel.append(mel_)\n",
        "    mel = np.array(mel)\n",
        "    mel = librosa.power_to_db(mel, ref = np.max)\n",
        "    return ((mel - mel.min())/(mel.max()-mel.min()))"
      ],
      "metadata": {
        "id": "tGJyZXbvBnNW"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def std_scaling(data, sr = 16000, n_fft = 1024, win_length = 1024, hop_length = 160, n_mels = 64):\n",
        "    mel = []\n",
        "    for i in tqdm(data):\n",
        "        mel_ = librosa.feature.melspectrogram(i[:80000], sr = sr, n_fft = n_fft, win_length = win_length, hop_length = hop_length, n_mels = n_mels)\n",
        "        mel.append(mel_)\n",
        "    mel = np.array(mel)\n",
        "    mel = librosa.power_to_db(mel, ref = np.max)\n",
        "    return ((mel-mel.mean())/mel.std())"
      ],
      "metadata": {
        "id": "qxwVrss5Cjq5"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "train_m = minmax_scaling(data = X_train)\n",
        "test_m = minmax_scaling(data = X_test)\n",
        "\n",
        "train_m = train_m.reshape(-1, train_m.shape[1], train_m.shape[2], 1)\n",
        "test_m = test_m.reshape(-1, test_m.shape[1], test_m.shape[2], 1)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "whTg0fwHClWh",
        "outputId": "77565abe-bae4-4a04-d965-627f9cdfa729"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "100%|██████████| 6000/6000 [03:08<00:00, 31.89it/s]\n",
            "100%|██████████| 6100/6100 [01:30<00:00, 67.74it/s]\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "train_s = std_scaling(data = X_train)\n",
        "test_s = std_scaling(data = X_test)\n",
        "\n",
        "train_s = train_s.reshape(-1, train_s.shape[1], train_s.shape[2], 1)\n",
        "test_s = test_s.reshape(-1, test_s.shape[1], test_s.shape[2], 1)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "LK6NOM2DCobC",
        "outputId": "2d331ca5-ec0b-472b-8a7e-aef57541cbad"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "100%|██████████| 6000/6000 [01:33<00:00, 63.91it/s]\n",
            "100%|██████████| 6100/6100 [01:26<00:00, 70.48it/s]\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "np.save(main_path+'/train_m.npy',arr=train_m)\n",
        "np.save(main_path+'/test_m.npy',arr=test_m)\n",
        "\n",
        "np.save(main_path+'/train_s.npy',arr=train_s)\n",
        "np.save(main_path+'/test_s.npy',arr=test_s)"
      ],
      "metadata": {
        "id": "xDbnhHEzKMoT"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "delete([train_m, test_m, train_s, test_s])"
      ],
      "metadata": {
        "id": "RFQ9e82dK1Ny"
      },
      "execution_count": 4,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#음성데이터에 맞게끔 믹스업과 랜덤이래이져를 수정하여 썼습니다.\n",
        "def get_random_eraser(p=0.5, s_l=0.02, s_h=0.4, r_1=0.3, r_2=1/0.3, v_l=0, v_h=255):\n",
        "    def eraser(input_img):\n",
        "        img_h, img_w, _ = input_img.shape\n",
        "        p_1 = np.random.rand()\n",
        "\n",
        "        if p_1 > p:\n",
        "            return input_img\n",
        "\n",
        "        while True:\n",
        "            s = np.random.uniform(s_l, s_h) * img_h * img_w\n",
        "            r = np.random.uniform(r_1, r_2)\n",
        "            w = int(np.sqrt(s / r))\n",
        "            h = int(np.sqrt(s * r))\n",
        "            left = np.random.randint(0, img_w)\n",
        "            top = np.random.randint(0, img_h)\n",
        "\n",
        "            if left + w <= img_w and top + h <= img_h:\n",
        "                break\n",
        "\n",
        "        c = np.array([0],dtype=np.float32)\n",
        "        input_img[top:top + h, left:left + w, :] = c\n",
        "\n",
        "        return input_img\n",
        "\n",
        "    return eraser"
      ],
      "metadata": {
        "id": "VPyEN36lLkPG"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "class MixupGenerator():\n",
        "    def __init__(self, X_train,y_train, batch_size=32, alpha=0.05, beta=0.2, shuffle=True, datagen=None):\n",
        "        self.X_train = X_train\n",
        "        self.y_train = y_train\n",
        "        self.batch_size = batch_size\n",
        "        self.alpha = alpha\n",
        "        self.beta = beta\n",
        "        self.shuffle = shuffle\n",
        "        self.sample_num = len(X_train)\n",
        "        self.datagen = datagen\n",
        "\n",
        "    def __call__(self):\n",
        "        while True:\n",
        "            indexes = self.__get_exploration_order()\n",
        "            itr_num = int(len(indexes) // (self.batch_size * 2))\n",
        "\n",
        "            for i in range(itr_num):\n",
        "                batch_ids = indexes[i * self.batch_size * 2:(i + 1) * self.batch_size * 2]\n",
        "                X, y = self.__data_generation(batch_ids)\n",
        "\n",
        "                yield X, y\n",
        "\n",
        "    def __get_exploration_order(self):\n",
        "        indexes = np.arange(self.sample_num)\n",
        "\n",
        "        if self.shuffle:\n",
        "            np.random.shuffle(indexes)\n",
        "\n",
        "        return indexes\n",
        "\n",
        "    def __data_generation(self, batch_ids):\n",
        "        _, h, w, c = self.X_train.shape\n",
        "        rlambda=np.random.uniform(self.beta, self.alpha)\n",
        "        rlambda = np.round(rlambda,2)\n",
        "        l = np.random.beta(rlambda, rlambda, self.batch_size)\n",
        "        X_l = l.reshape(self.batch_size, 1, 1, 1)\n",
        "        y_l = l.reshape(self.batch_size, 1)\n",
        "\n",
        "        X1 = self.X_train[batch_ids[:self.batch_size]]\n",
        "        X2 = self.X_train[batch_ids[self.batch_size:]]\n",
        "        X = X1 * X_l + X2 * (1 - X_l)\n",
        "\n",
        "        if self.datagen:\n",
        "            for i in range(self.batch_size):\n",
        "                X[i] = self.datagen.random_transform(X[i])\n",
        "                X[i] = self.datagen.standardize(X[i])\n",
        "\n",
        "        if isinstance(self.y_train, list):\n",
        "            y = []\n",
        "\n",
        "            for y_train_ in self.y_train:\n",
        "                y1 = y_train_[batch_ids[:self.batch_size]]\n",
        "                y2 = y_train_[batch_ids[self.batch_size:]]\n",
        "                y.append(y1 * y_l + y2 * (1 - y_l))\n",
        "        else:\n",
        "            y1 = self.y_train[batch_ids[:self.batch_size]]\n",
        "            y2 = self.y_train[batch_ids[self.batch_size:]]\n",
        "            y = y1 * y_l + y2 * (1 - y_l)\n",
        "\n",
        "        return X, y"
      ],
      "metadata": {
        "id": "6hwDq8mULDWg"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def first_block(input_, units = 32, dropout_rate = 0.5):\n",
        "    x = Convolution2D(units, 3, padding =\"same\", activation = \"relu\")(input_)\n",
        "    x = BatchNormalization()(x)\n",
        "    x_res = x\n",
        "    x = Convolution2D(units, 3, padding =\"same\", activation = \"relu\")(x)\n",
        "    x = BatchNormalization()(x)\n",
        "    x = Convolution2D(units, 3, padding =\"same\", activation = \"relu\")(x)\n",
        "    x = BatchNormalization()(x)\n",
        "    x = Add()([x, x_res])\n",
        "    x = AveragePooling2D()(x)\n",
        "    x = Dropout(rate=dropout_rate)(x)\n",
        "    \n",
        "    return x\n",
        "\n",
        "def second_block(input_, units = 64, dropout_rate = 0.5):\n",
        "    x = Convolution2D(units, 1, padding =\"same\", activation = \"relu\")(input_)\n",
        "    x = Convolution2D(units, 3, padding =\"same\", activation = \"relu\")(x)\n",
        "    x = Convolution2D(units * 4, 1, padding =\"same\", activation = \"relu\")(x)\n",
        "    x = BatchNormalization()(x)\n",
        "    x_res = x\n",
        "    x = Convolution2D(units, 1, padding =\"same\", activation = \"relu\")(x)\n",
        "    x = Convolution2D(units, 3, padding =\"same\", activation = \"relu\")(x)\n",
        "    x = Convolution2D(units * 4, 1, padding =\"same\", activation = \"relu\")(x)\n",
        "    x = BatchNormalization()(x)\n",
        "    x = Convolution2D(units, 1, padding = \"same\", activation = \"relu\")(x)\n",
        "    x = Convolution2D(units, 3, padding =\"same\", activation = \"relu\")(x)\n",
        "    x = Convolution2D(units * 4, 1, padding = \"same\", activation = \"relu\")(x)\n",
        "    x = BatchNormalization()(x)\n",
        "    x = Add()([x, x_res])\n",
        "    x = AveragePooling2D()(x)\n",
        "    x = Dropout(rate=dropout_rate)(x)\n",
        "    \n",
        "    return x"
      ],
      "metadata": {
        "id": "EcitsMIBLIBI"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def build_fn():\n",
        "    dropout_rate = 0.3\n",
        "    \n",
        "    in_ = Input(shape = (X_train.shape[1:]))\n",
        "    \n",
        "    block_01 = first_block(in_, units = 32, dropout_rate = dropout_rate)\n",
        "    block_02 = first_block(block_01, units = 64, dropout_rate = dropout_rate)\n",
        "    block_03 = first_block(block_02, units = 128, dropout_rate = dropout_rate)\n",
        "\n",
        "    block_04 = second_block(block_03, units = 64, dropout_rate = dropout_rate)\n",
        "    block_05 = second_block(block_04, units = 128, dropout_rate = dropout_rate)\n",
        "\n",
        "    x = Flatten()(block_05)\n",
        "\n",
        "    x = Dropout(rate = dropout_rate)(x)\n",
        "\n",
        "    x = Dense(units = 64, activation = \"relu\")(x)\n",
        "    x = BatchNormalization()(x)\n",
        "    x_res = x\n",
        "    x = Dropout(rate = dropout_rate)(x)\n",
        "\n",
        "    x = Dense(units = 64, activation = \"relu\")(x)\n",
        "    x = BatchNormalization()(x)\n",
        "    x = Add()([x_res, x])\n",
        "    x = Dropout(rate = dropout_rate)(x)\n",
        "\n",
        "    model_out = Dense(units = 6, activation = 'softmax')(x)\n",
        "    model = Model(in_, model_out)\n",
        "    return model"
      ],
      "metadata": {
        "id": "nCZZan-OLLCI"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from keras.preprocessing.image import ImageDataGenerator\n",
        "imagegen = ImageDataGenerator(\n",
        "    width_shift_range=0.04,\n",
        "    height_shift_range=0.04,\n",
        "    preprocessing_function=get_random_eraser())"
      ],
      "metadata": {
        "id": "zlOxdSQILMY9"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "temp_y = np.concatenate((np.zeros(1000, dtype = np.int),\n",
        "                        np.ones(1000, dtype = np.int),\n",
        "                         np.ones(1000, dtype = np.int) * 2,\n",
        "                         np.ones(1000, dtype = np.int) * 3,\n",
        "                         np.ones(1000, dtype = np.int) * 4,\n",
        "                         np.ones(1000, dtype = np.int) * 5), axis = 0)"
      ],
      "metadata": {
        "id": "XqFtCxeNLOPw"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## model train function"
      ],
      "metadata": {
        "id": "Ia_V2UMsr_e9"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def model_train(k, patience, alpha, epochs):\n",
        "  for person in range(3):\n",
        "    split = KFold(n_splits = k, shuffle = True, random_state = 10)\n",
        "    early_stopping = EarlyStopping(patience=patience)\n",
        "    idx = [k+person for k in range(0, len(X_train), 3)][:-1]\n",
        "    X_train = X_train[idx]\n",
        "    temp_y = y_train[idx]\n",
        "\n",
        "    for e , (train_idx, val_idx) in enumerate(split.split(X_train, temp_y)):\n",
        "    #  if e>=7: #메모리 때문에 끊어서 사용\n",
        "      X_train, y_train = X_train[train_idx], y_train[train_idx]\n",
        "      x_val, y_val = X_train[val_idx], y_train[val_idx]\n",
        "      training_generator = MixupGenerator(X_train, y_train, batch_size=128,datagen=imagegen)()\n",
        "      checkpoint = ModelCheckpoint(main_path+person+f'/check{e}.h5',save_best_only=True)\n",
        "      model = build_fn()\n",
        "      model.compile(optimizer = tf.keras.optimizers.Adam(alpha),\n",
        "                    loss = keras.losses.CategoricalCrossentropy(),\n",
        "                    metrics=['accuracy'])\n",
        "      history = model.fit(training_generator, steps_per_epoch=x_train.shape[0] //128, \n",
        "                          validation_data = (x_val, y_val), epochs = epochs, callbacks=[early_stopping, checkpoint])\n",
        "      gc.collect()\n",
        "      print(\"fold :\", e)\n",
        "      print(\"*******************************************************************\")"
      ],
      "metadata": {
        "id": "BrzMMAijr_Hr"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "##standardization된 data로 train"
      ],
      "metadata": {
        "id": "7U0UxFD6qFif"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "X_train = np.load(main_path+'/train_s.npy')\n",
        "model_train(5, 10, 0.002, 30)"
      ],
      "metadata": {
        "id": "IFxnZBZ0L3t-"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# model_train(5, 10, 0.002, 30)\n",
        "# model_train(5, 10, 0.002, 30)"
      ],
      "metadata": {
        "id": "P6cLUdFTuCjc"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## min-maxscaled된 data로 train"
      ],
      "metadata": {
        "id": "hhOuERbKqJx-"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "delete(X_train)\n",
        "X_train= np.load(main_path+'/train_m.npy')\n",
        "model_train(5, 10, 0.002, 30)"
      ],
      "metadata": {
        "id": "efSrEf49Sc-3"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## 테스트"
      ],
      "metadata": {
        "id": "_VxDq5TTq4FG"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### 모델 예측"
      ],
      "metadata": {
        "id": "cutx4HGCq9rY"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def predict(list, list_, n):\n",
        "  early_stopping = EarlyStopping(patience=10)\n",
        "\n",
        "  for e in range(n):\n",
        "    model = build_fn()\n",
        "    model.load_weights(main_path+f'/checkcheck{e}.h5')\n",
        "    print(\"*******************************************************************\")\n",
        "    list.append(model.predict(X_test))\n",
        "    list_.append(np.argmax(model.predict(X_test), axis = 1))\n",
        "    print(\"*******************************************************************\")"
      ],
      "metadata": {
        "id": "oM5tCOelHbUt"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "#### min-max data 예측\n",
        "\n"
      ],
      "metadata": {
        "id": "Rmgh4q_QIZHt"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "X_test= np.load(main_path+'/test_m.npy')\n",
        "pred2 = []\n",
        "pred2_ = []\n",
        "predict(pred2, pred2_, 10)"
      ],
      "metadata": {
        "id": "QLb3DztvrVTZ"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "#### std 예측\n",
        "\n"
      ],
      "metadata": {
        "id": "aw12Ii6WrFav"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "X_test = np.load(main_path+'/test_s.npy')\n",
        "pred1 = []\n",
        "pred1_ = []\n",
        "predict(pred1, pred1_, 5)"
      ],
      "metadata": {
        "id": "pDlB5V9Kq29B"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## 6100"
      ],
      "metadata": {
        "id": "xrmiuiJArq2J"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "test_ = pd.DataFrame(index = range(0, 6100), columns = [\"path\", \"id\"])\n",
        "test_[\"path\"] = [main_path + f\"/open/test/{i+1}.wav\" for i in range(6100)]\n",
        "test_[\"id\"] =[i+1 for i in range(6100)]\n",
        "test_.head()"
      ],
      "metadata": {
        "id": "AaQQBTYQrZ7w"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "sample_submission = pd.read_csv(main_path + \"/open/sample_submission.csv\")"
      ],
      "metadata": {
        "id": "61rFSO5wrbSr"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "result1 = pd.concat([test_, pd.DataFrame(np.mean(pred1, axis = 0))], axis = 1).iloc[:, 1:]\n",
        "result1 = pd.merge(sample_submission[\"id\"], result1)\n",
        "result1.columns = sample_submission.columns"
      ],
      "metadata": {
        "id": "CNUesNPWrccM"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "result2 = pd.concat([test_, pd.DataFrame(np.mean(pred2, axis = 0))], axis = 1).iloc[:, 1:]\n",
        "result2 = pd.merge(sample_submission[\"id\"], result2)\n",
        "result2.columns = sample_submission.columns"
      ],
      "metadata": {
        "id": "XlkhG8PSrdfp"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "result=result1.copy()\n",
        "result.iloc[:,1:]=(result1.iloc[:,1:]+result2.iloc[:,1:])/2\n",
        "result"
      ],
      "metadata": {
        "id": "rdyskqVjre5p"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "result.to_csv(main_path+\"/audio.csv\", index = False)"
      ],
      "metadata": {
        "id": "962xtTNZrfsC"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}