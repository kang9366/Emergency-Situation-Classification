{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU",
    "gpuClass": "standard"
  },
  "cells": [
    {
      "cell_type": "code",
      "execution_count": 14,
      "metadata": {
        "id": "1Orvat767_Id"
      },
      "outputs": [],
      "source": [
        "import numpy as np\n",
        "import tensorflow as tf\n",
        "from tensorflow import keras\n",
        "import matplotlib.pyplot as plt\n",
        "from sklearn.model_selection import train_test_split\n",
        "from tensorflow.keras.optimizers import SGD, Adam, RMSprop, Adadelta\n",
        "from tensorflow.keras.utils import to_categorical\n",
        "from tensorflow.keras.models import Sequential\n",
        "from tensorflow.keras.layers import Dense, Dropout, Flatten, Activation, MaxPool2D, BatchNormalization, Conv2D, MaxPooling2D\n",
        "from tensorflow.keras.preprocessing.image import ImageDataGenerator\n",
        "from tensorflow.keras.models import Model\n",
        "from tensorflow.keras.layers import (Input, Convolution2D, BatchNormalization, Flatten,\n",
        "                                     Dropout, Dense, AveragePooling2D,MaxPooling2D, Add, GlobalAveragePooling2D)\n",
        "from tensorflow.keras.callbacks import EarlyStopping,ModelCheckpoint\n",
        "\n",
        "from sklearn.model_selection import KFold,StratifiedKFold\n",
        "from sklearn.metrics import accuracy_score\n",
        "\n",
        "import pandas as pd\n",
        "import numpy as np\n",
        "import os\n",
        "import shutil\n",
        "from tqdm import tqdm\n",
        "from glob import glob\n",
        "import tensorflow\n",
        "import librosa\n",
        "import warnings\n",
        "import gc\n",
        "warnings.filterwarnings(\"ignore\")"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from google.colab import drive\n",
        "drive.mount('/content/drive')"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "2QetPOk38rFq",
        "outputId": "9fe50c10-8d47-47d7-a8fa-67bfbe1712c4"
      },
      "execution_count": 15,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Drive already mounted at /content/drive; to attempt to forcibly remount, call drive.mount(\"/content/drive\", force_remount=True).\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "cd /content/drive/MyDrive"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "rMjTEpP8-VcZ",
        "outputId": "40782ec8-a816-46ad-eef8-61183896a33d"
      },
      "execution_count": 16,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "/content/drive/MyDrive\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "main_path = 'Data'"
      ],
      "metadata": {
        "id": "9Qxxu3jwFrhk"
      },
      "execution_count": 17,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def load_data(paths):\n",
        "    result = []\n",
        "    for path in tqdm(paths):\n",
        "        data, sr = librosa.load(path, sr = 16000)\n",
        "        result.append(data)\n",
        "    result = np.array(result) \n",
        "    return result"
      ],
      "metadata": {
        "id": "mEQFJoEG-Oji"
      },
      "execution_count": 18,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "africa_train_data = np.load(main_path +\"/npy_data/africa_npy.npy\", allow_pickle = True)\n",
        "print(len(africa_train_data ))\n",
        "australia_train_data = np.load(main_path +\"/npy_data/australia_npy.npy\", allow_pickle = True)\n",
        "print(len(australia_train_data ))\n",
        "canada_train_data = np.load(main_path +\"/npy_data/canada_npy.npy\", allow_pickle = True)\n",
        "print(len(canada_train_data ))\n",
        "england_train_data = np.load(main_path +\"/npy_data/england_npy.npy\", allow_pickle = True)\n",
        "print(len(england_train_data ))\n",
        "hongkong_train_data = np.load(main_path +\"/npy_data/hongkong_npy.npy\", allow_pickle = True)\n",
        "print(len(hongkong_train_data ))\n",
        "us_train_data = np.load(main_path +\"/npy_data/us_npy.npy\", allow_pickle = True)\n",
        "print(len(us_train_data ))\n",
        "test_data = np.load(main_path +\"/npy_data/test_npy.npy\", allow_pickle = True)\n",
        "print(len(test_data))\n",
        "train_data_list = [africa_train_data, australia_train_data, canada_train_data, england_train_data, hongkong_train_data, us_train_data]"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "UpT0hcyy-n5A",
        "outputId": "dea2e449-a836-4a25-b538-9e50d3e9e102"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "1000\n",
            "1000\n",
            "1000\n",
            "1000\n",
            "1000\n",
            "1000\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "print(len(test_data))"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "YHipFLF6mki7",
        "outputId": "43fea185-d43e-48c9-be16-a3da60024144"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "6100\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "train_x = np.concatenate(train_data_list, axis= 0)"
      ],
      "metadata": {
        "id": "vTMX25KO-xfT"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "test_x = np.array(test_data)"
      ],
      "metadata": {
        "id": "4BHMHvzh_REY"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# train_data의 label을 생성해 줍니다.\n",
        "train_y =np.concatenate((np.zeros(1000, dtype = np.int),\n",
        "                        np.ones(1000, dtype = np.int),\n",
        "                         np.ones(1000, dtype = np.int) * 2,\n",
        "                         np.ones(1000, dtype = np.int) * 3,\n",
        "                         np.ones(1000, dtype = np.int) * 4,\n",
        "                         np.ones(1000, dtype = np.int) * 5), axis = 0)"
      ],
      "metadata": {
        "id": "-msvcwQX_S_r"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "train_y = to_categorical(train_y)"
      ],
      "metadata": {
        "id": "7E_YxTn2_UWV"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "del africa_train_data\n",
        "del australia_train_data\n",
        "del canada_train_data \n",
        "del england_train_data\n",
        "del hongkong_train_data \n",
        "del us_train_data \n",
        "del test_data \n",
        "del train_data_list \n",
        "gc.collect()"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "_WDw6HIX_Vr8",
        "outputId": "cf692d2a-417c-4d2b-9dfb-8d3cb94e6525"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "45"
            ]
          },
          "metadata": {},
          "execution_count": 27
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Normalization / Standardization "
      ],
      "metadata": {
        "id": "vmJglZujBsVn"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def get_minmax(data, sr = 16000, n_fft = 1024, win_length = 1024, hop_length = 160, n_mels = 64):\n",
        "    mel = []\n",
        "    for i in tqdm(data):\n",
        "        mel_ = librosa.feature.melspectrogram(i[:80000], sr = sr, n_fft = n_fft, win_length = win_length, hop_length = hop_length, n_mels = n_mels)\n",
        "        mel.append(mel_)\n",
        "    mel = np.array(mel)\n",
        "    mel = librosa.power_to_db(mel, ref = np.max)\n",
        "\n",
        "    mel_min = mel.min()\n",
        "    mel_max = mel.max()\n",
        "    mel = (mel - mel_min) / (mel_max-mel_min)\n",
        "    return mel"
      ],
      "metadata": {
        "id": "tGJyZXbvBnNW"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def get_std(data, sr = 16000, n_fft = 1024, win_length = 1024, hop_length = 160, n_mels = 64):\n",
        "    mel = []\n",
        "    for i in tqdm(data):\n",
        "        mel_ = librosa.feature.melspectrogram(i[:80000], sr = sr, n_fft = n_fft, win_length = win_length, hop_length = hop_length, n_mels = n_mels)\n",
        "        mel.append(mel_)\n",
        "    mel = np.array(mel)\n",
        "    mel = librosa.power_to_db(mel, ref = np.max)\n",
        "\n",
        "    mel_mean = mel.mean()\n",
        "    mel_std = mel.std()\n",
        "    mel = (mel - mel_mean) / mel_std\n",
        "    return mel"
      ],
      "metadata": {
        "id": "qxwVrss5Cjq5"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "train_m = get_minmax(data = train_x)\n",
        "test_m = get_minmax(data = test_x)\n",
        "\n",
        "train_m = train_m.reshape(-1, train_m.shape[1], train_m.shape[2], 1)\n",
        "test_m = test_m.reshape(-1, test_m.shape[1], test_m.shape[2], 1)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "whTg0fwHClWh",
        "outputId": "77565abe-bae4-4a04-d965-627f9cdfa729"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "100%|██████████| 6000/6000 [03:08<00:00, 31.89it/s]\n",
            "100%|██████████| 6100/6100 [01:30<00:00, 67.74it/s]\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "train_s = get_std(data = train_x)\n",
        "test_s = get_std(data = test_x)\n",
        "\n",
        "train_s = train_s.reshape(-1, train_s.shape[1], train_s.shape[2], 1)\n",
        "test_s = test_s.reshape(-1, test_s.shape[1], test_s.shape[2], 1)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "LK6NOM2DCobC",
        "outputId": "2d331ca5-ec0b-472b-8a7e-aef57541cbad"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "100%|██████████| 6000/6000 [01:33<00:00, 63.91it/s]\n",
            "100%|██████████| 6100/6100 [01:26<00:00, 70.48it/s]\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "np.save(main_path+'/train_m.npy',arr=train_m)\n",
        "np.save(main_path+'/test_m.npy',arr=test_m)\n",
        "\n",
        "np.save(main_path+'/train_s.npy',arr=train_s)\n",
        "np.save(main_path+'/test_s.npy',arr=test_s)\n"
      ],
      "metadata": {
        "id": "xDbnhHEzKMoT"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "del train_m\n",
        "del test_m\n",
        "del train_s\n",
        "del test_s\n",
        "gc.collect()"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "RFQ9e82dK1Ny",
        "outputId": "82594365-932b-4c02-b389-2852814c1458"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "0"
            ]
          },
          "metadata": {},
          "execution_count": 33
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "#음성데이터에 맞게끔 믹스업과 랜덤이래이져를 수정하여 썼습니다.\n",
        "def get_random_eraser(p=0.5, s_l=0.02, s_h=0.4, r_1=0.3, r_2=1/0.3, v_l=0, v_h=255):\n",
        "    def eraser(input_img):\n",
        "        img_h, img_w, _ = input_img.shape\n",
        "        p_1 = np.random.rand()\n",
        "\n",
        "        if p_1 > p:\n",
        "            return input_img\n",
        "\n",
        "        while True:\n",
        "            s = np.random.uniform(s_l, s_h) * img_h * img_w\n",
        "            r = np.random.uniform(r_1, r_2)\n",
        "            w = int(np.sqrt(s / r))\n",
        "            h = int(np.sqrt(s * r))\n",
        "            left = np.random.randint(0, img_w)\n",
        "            top = np.random.randint(0, img_h)\n",
        "\n",
        "            if left + w <= img_w and top + h <= img_h:\n",
        "                break\n",
        "\n",
        "        c = np.array([0],dtype=np.float32)\n",
        "        input_img[top:top + h, left:left + w, :] = c\n",
        "\n",
        "        return input_img\n",
        "\n",
        "    return eraser"
      ],
      "metadata": {
        "id": "VPyEN36lLkPG"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "class MixupGenerator():\n",
        "    def __init__(self, X_train,y_train, batch_size=32, alpha=0.05, beta=0.2, shuffle=True, datagen=None):\n",
        "        self.X_train = X_train\n",
        "        self.y_train = y_train\n",
        "        self.batch_size = batch_size\n",
        "        self.alpha = alpha\n",
        "        self.beta = beta\n",
        "        self.shuffle = shuffle\n",
        "        self.sample_num = len(X_train)\n",
        "        self.datagen = datagen\n",
        "\n",
        "    def __call__(self):\n",
        "        while True:\n",
        "            indexes = self.__get_exploration_order()\n",
        "            itr_num = int(len(indexes) // (self.batch_size * 2))\n",
        "\n",
        "            for i in range(itr_num):\n",
        "                batch_ids = indexes[i * self.batch_size * 2:(i + 1) * self.batch_size * 2]\n",
        "                X, y = self.__data_generation(batch_ids)\n",
        "\n",
        "                yield X, y\n",
        "\n",
        "    def __get_exploration_order(self):\n",
        "        indexes = np.arange(self.sample_num)\n",
        "\n",
        "        if self.shuffle:\n",
        "            np.random.shuffle(indexes)\n",
        "\n",
        "        return indexes\n",
        "\n",
        "    def __data_generation(self, batch_ids):\n",
        "        _, h, w, c = self.X_train.shape\n",
        "        rlambda=np.random.uniform(self.beta, self.alpha)\n",
        "        rlambda = np.round(rlambda,2)\n",
        "        l = np.random.beta(rlambda, rlambda, self.batch_size)\n",
        "        X_l = l.reshape(self.batch_size, 1, 1, 1)\n",
        "        y_l = l.reshape(self.batch_size, 1)\n",
        "\n",
        "        X1 = self.X_train[batch_ids[:self.batch_size]]\n",
        "        X2 = self.X_train[batch_ids[self.batch_size:]]\n",
        "        X = X1 * X_l + X2 * (1 - X_l)\n",
        "\n",
        "        if self.datagen:\n",
        "            for i in range(self.batch_size):\n",
        "                X[i] = self.datagen.random_transform(X[i])\n",
        "                X[i] = self.datagen.standardize(X[i])\n",
        "\n",
        "        if isinstance(self.y_train, list):\n",
        "            y = []\n",
        "\n",
        "            for y_train_ in self.y_train:\n",
        "                y1 = y_train_[batch_ids[:self.batch_size]]\n",
        "                y2 = y_train_[batch_ids[self.batch_size:]]\n",
        "                y.append(y1 * y_l + y2 * (1 - y_l))\n",
        "        else:\n",
        "            y1 = self.y_train[batch_ids[:self.batch_size]]\n",
        "            y2 = self.y_train[batch_ids[self.batch_size:]]\n",
        "            y = y1 * y_l + y2 * (1 - y_l)\n",
        "\n",
        "        return X, y"
      ],
      "metadata": {
        "id": "6hwDq8mULDWg"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def block(input_, units = 32, dropout_rate = 0.5):\n",
        "    \n",
        "    x = Convolution2D(units, 3, padding =\"same\", activation = \"relu\")(input_)\n",
        "    x = BatchNormalization()(x)\n",
        "    x_res = x\n",
        "    x = Convolution2D(units, 3, padding =\"same\", activation = \"relu\")(x)\n",
        "    x = BatchNormalization()(x)\n",
        "    x = Convolution2D(units, 3, padding =\"same\", activation = \"relu\")(x)\n",
        "    x = BatchNormalization()(x)\n",
        "    x = Add()([x, x_res])\n",
        "    x = AveragePooling2D()(x)\n",
        "    x = Dropout(rate=dropout_rate)(x)\n",
        "    \n",
        "    return x\n",
        "\n",
        "def second_block(input_, units = 64, dropout_rate = 0.5):\n",
        "    \n",
        "    x = Convolution2D(units, 1, padding =\"same\", activation = \"relu\")(input_)\n",
        "    x = Convolution2D(units, 3, padding =\"same\", activation = \"relu\")(x)\n",
        "    x = Convolution2D(units * 4, 1, padding =\"same\", activation = \"relu\")(x)\n",
        "    x = BatchNormalization()(x)\n",
        "    x_res = x\n",
        "    x = Convolution2D(units, 1, padding =\"same\", activation = \"relu\")(x)\n",
        "    x = Convolution2D(units, 3, padding =\"same\", activation = \"relu\")(x)\n",
        "    x = Convolution2D(units * 4, 1, padding =\"same\", activation = \"relu\")(x)\n",
        "    x = BatchNormalization()(x)\n",
        "    x = Convolution2D(units, 1, padding = \"same\", activation = \"relu\")(x)\n",
        "    x = Convolution2D(units, 3, padding =\"same\", activation = \"relu\")(x)\n",
        "    x = Convolution2D(units * 4, 1, padding = \"same\", activation = \"relu\")(x)\n",
        "    x = BatchNormalization()(x)\n",
        "    x = Add()([x, x_res])\n",
        "    x = AveragePooling2D()(x)\n",
        "    x = Dropout(rate=dropout_rate)(x)\n",
        "    \n",
        "    return x"
      ],
      "metadata": {
        "id": "EcitsMIBLIBI"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def build_fn():\n",
        "    dropout_rate = 0.3\n",
        "    \n",
        "    in_ = Input(shape = (train_x.shape[1:]))\n",
        "    \n",
        "    block_01 = block(in_, units = 32, dropout_rate = dropout_rate)\n",
        "    block_02 = block(block_01, units = 64, dropout_rate = dropout_rate)\n",
        "    block_03 = block(block_02, units = 128, dropout_rate = dropout_rate)\n",
        "\n",
        "    block_04 = second_block(block_03, units = 64, dropout_rate = dropout_rate)\n",
        "    block_05 = second_block(block_04, units = 128, dropout_rate = dropout_rate)\n",
        "\n",
        "    x = Flatten()(block_05)\n",
        "\n",
        "    x = Dropout(rate = dropout_rate)(x)\n",
        "\n",
        "    x = Dense(units = 64, activation = \"relu\")(x)\n",
        "    x = BatchNormalization()(x)\n",
        "    x_res = x\n",
        "    x = Dropout(rate = dropout_rate)(x)\n",
        "\n",
        "    x = Dense(units = 64, activation = \"relu\")(x)\n",
        "    x = BatchNormalization()(x)\n",
        "    x = Add()([x_res, x])\n",
        "    x = Dropout(rate = dropout_rate)(x)\n",
        "\n",
        "    model_out = Dense(units = 6, activation = 'softmax')(x)\n",
        "    model = Model(in_, model_out)\n",
        "    return model"
      ],
      "metadata": {
        "id": "nCZZan-OLLCI"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from keras.preprocessing.image import ImageDataGenerator\n",
        "imagegen = ImageDataGenerator(\n",
        "    width_shift_range=0.04,\n",
        "    height_shift_range=0.04,\n",
        "    preprocessing_function=get_random_eraser())"
      ],
      "metadata": {
        "id": "zlOxdSQILMY9"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "temp_y = np.concatenate((np.zeros(1000, dtype = np.int),\n",
        "                        np.ones(1000, dtype = np.int),\n",
        "                         np.ones(1000, dtype = np.int) * 2,\n",
        "                         np.ones(1000, dtype = np.int) * 3,\n",
        "                         np.ones(1000, dtype = np.int) * 4,\n",
        "                         np.ones(1000, dtype = np.int) * 5), axis = 0)"
      ],
      "metadata": {
        "id": "XqFtCxeNLOPw"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "##standardization된 data로 train"
      ],
      "metadata": {
        "id": "7U0UxFD6qFif"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "train_x= np.load(main_path+'/train_s.npy')"
      ],
      "metadata": {
        "id": "IFxnZBZ0L3t-"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#Standardization 모델\n",
        "for person in range(3):\n",
        "  split = KFold(n_splits = 5, shuffle = True, random_state = 10)\n",
        "  early_stopping = EarlyStopping(patience=10)\n",
        "  idx = [k+person for k in range(0, len(train_x), 3)][:-1]\n",
        "  train_x=train_x[idx]\n",
        "  temp_y=train_y[idx]\n",
        "  for e , (train_idx, val_idx) in enumerate(split.split(train_x, temp_y)):\n",
        "  #  if e>=7: #메모리 때문에 끊어서 사용\n",
        "      \n",
        "      x_train, y_train = train_x[train_idx], train_y[train_idx]\n",
        "      x_val, y_val = train_x[val_idx], train_y[val_idx]\n",
        "      training_generator = MixupGenerator(x_train, y_train, batch_size=128,datagen=imagegen)()\n",
        "      checkpoint = ModelCheckpoint(main_path+person+f'/check{e}.h5',save_best_only=True)\n",
        "      model = build_fn()\n",
        "      model.compile(optimizer = tf.keras.optimizers.Adam(0.002),\n",
        "                  loss = keras.losses.CategoricalCrossentropy(),\n",
        "                  )\n",
        "\n",
        "      history = model.fit(training_generator, steps_per_epoch=x_train.shape[0] //128, validation_data = (x_val, y_val), epochs = 30, callbacks=[early_stopping, checkpoint])\n",
        "      gc.collect()\n",
        "      print(\"fold :\", e)\n",
        "      print(\"*******************************************************************\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 423
        },
        "id": "ZCTr1AK2L7mp",
        "outputId": "2accff7f-10d0-4526-9637-c38387610438"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "error",
          "ename": "KeyboardInterrupt",
          "evalue": "ignored",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-43-c32ba6597730>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m     18\u001b[0m                   )\n\u001b[1;32m     19\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 20\u001b[0;31m       \u001b[0mhistory\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mmodel\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfit\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtraining_generator\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0msteps_per_epoch\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mx_train\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mshape\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m//\u001b[0m\u001b[0;36m128\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mvalidation_data\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0mx_val\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0my_val\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mepochs\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;36m30\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcallbacks\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mearly_stopping\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcheckpoint\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     21\u001b[0m       \u001b[0mgc\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcollect\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     22\u001b[0m       \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"fold :\"\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0me\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.8/dist-packages/keras/utils/traceback_utils.py\u001b[0m in \u001b[0;36merror_handler\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m     62\u001b[0m     \u001b[0mfiltered_tb\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     63\u001b[0m     \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 64\u001b[0;31m       \u001b[0;32mreturn\u001b[0m \u001b[0mfn\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     65\u001b[0m     \u001b[0;32mexcept\u001b[0m \u001b[0mException\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0me\u001b[0m\u001b[0;34m:\u001b[0m  \u001b[0;31m# pylint: disable=broad-except\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     66\u001b[0m       \u001b[0mfiltered_tb\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0m_process_traceback_frames\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0me\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m__traceback__\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.8/dist-packages/keras/engine/training.py\u001b[0m in \u001b[0;36mfit\u001b[0;34m(self, x, y, batch_size, epochs, verbose, callbacks, validation_split, validation_data, shuffle, class_weight, sample_weight, initial_epoch, steps_per_epoch, validation_steps, validation_batch_size, validation_freq, max_queue_size, workers, use_multiprocessing)\u001b[0m\n\u001b[1;32m   1356\u001b[0m          \u001b[0mtraining_utils\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mRespectCompiledTrainableState\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1357\u001b[0m       \u001b[0;31m# Creates a `tf.data.Dataset` and handles batch and epoch iteration.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1358\u001b[0;31m       data_handler = data_adapter.get_data_handler(\n\u001b[0m\u001b[1;32m   1359\u001b[0m           \u001b[0mx\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mx\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1360\u001b[0m           \u001b[0my\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0my\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.8/dist-packages/keras/engine/data_adapter.py\u001b[0m in \u001b[0;36mget_data_handler\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m   1399\u001b[0m   \u001b[0;32mif\u001b[0m \u001b[0mgetattr\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m\"model\"\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m\"_cluster_coordinator\"\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1400\u001b[0m     \u001b[0;32mreturn\u001b[0m \u001b[0m_ClusterCoordinatorDataHandler\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1401\u001b[0;31m   \u001b[0;32mreturn\u001b[0m \u001b[0mDataHandler\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1402\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1403\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.8/dist-packages/keras/engine/data_adapter.py\u001b[0m in \u001b[0;36m__init__\u001b[0;34m(self, x, y, sample_weight, batch_size, steps_per_epoch, initial_epoch, epochs, shuffle, class_weight, max_queue_size, workers, use_multiprocessing, model, steps_per_execution, distribute)\u001b[0m\n\u001b[1;32m   1149\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1150\u001b[0m     \u001b[0madapter_cls\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mselect_data_adapter\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mx\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0my\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1151\u001b[0;31m     self._adapter = adapter_cls(\n\u001b[0m\u001b[1;32m   1152\u001b[0m         \u001b[0mx\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1153\u001b[0m         \u001b[0my\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.8/dist-packages/keras/engine/data_adapter.py\u001b[0m in \u001b[0;36m__init__\u001b[0;34m(self, x, y, sample_weights, workers, use_multiprocessing, max_queue_size, model, **kwargs)\u001b[0m\n\u001b[1;32m    798\u001b[0m     \u001b[0;31m# Since we have to know the dtype of the python generator when we build the\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    799\u001b[0m     \u001b[0;31m# dataset, we have to look at a batch to infer the structure.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 800\u001b[0;31m     \u001b[0mpeek\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mx\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_peek_and_restore\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mx\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    801\u001b[0m     \u001b[0mpeek\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_standardize_batch\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mpeek\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    802\u001b[0m     \u001b[0mpeek\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0m_process_tensorlike\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mpeek\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.8/dist-packages/keras/engine/data_adapter.py\u001b[0m in \u001b[0;36m_peek_and_restore\u001b[0;34m(x)\u001b[0m\n\u001b[1;32m    858\u001b[0m   \u001b[0;34m@\u001b[0m\u001b[0mstaticmethod\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    859\u001b[0m   \u001b[0;32mdef\u001b[0m \u001b[0m_peek_and_restore\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mx\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 860\u001b[0;31m     \u001b[0mpeek\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mnext\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mx\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    861\u001b[0m     \u001b[0;32mreturn\u001b[0m \u001b[0mpeek\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mitertools\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mchain\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mpeek\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mx\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    862\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m<ipython-input-35-ddbad7e35719>\u001b[0m in \u001b[0;36m__call__\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m     12\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0m__call__\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     13\u001b[0m         \u001b[0;32mwhile\u001b[0m \u001b[0;32mTrue\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 14\u001b[0;31m             \u001b[0mindexes\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m__get_exploration_order\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     15\u001b[0m             \u001b[0mitr_num\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mlen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mindexes\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m//\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mbatch_size\u001b[0m \u001b[0;34m*\u001b[0m \u001b[0;36m2\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     16\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m<ipython-input-35-ddbad7e35719>\u001b[0m in \u001b[0;36m__get_exploration_order\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m     25\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     26\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mshuffle\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 27\u001b[0;31m             \u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mrandom\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mshuffle\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mindexes\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     28\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     29\u001b[0m         \u001b[0;32mreturn\u001b[0m \u001b[0mindexes\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "del train_x\n",
        "gc.collect()"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "8IEINetWZluU",
        "outputId": "72725689-7490-438f-fe37-d4608cae4f0e"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "17663"
            ]
          },
          "metadata": {},
          "execution_count": 45
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## minmaxscaled된 data로 train"
      ],
      "metadata": {
        "id": "hhOuERbKqJx-"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "del train_x\n",
        "gc.collect()\n",
        "train_x= np.load(main_path+'/train_m.npy')"
      ],
      "metadata": {
        "id": "efSrEf49Sc-3"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#Standardization 모델\n",
        "for person in range(3):\n",
        "  split = KFold(n_splits = 5, shuffle = True, random_state = 10)\n",
        "  early_stopping = EarlyStopping(patience=10)\n",
        "  idx = [k+person for k in range(0, len(train_x), 3)][:-1]\n",
        "  train_x=train_x[idx]\n",
        "  temp_y=train_y[idx]\n",
        "  for e , (train_idx, val_idx) in enumerate(split.split(train_x, temp_y)):\n",
        "  #  if e>=7: #메모리 때문에 끊어서 사용\n",
        "      \n",
        "      x_train, y_train = train_x[train_idx], train_y[train_idx]\n",
        "      x_val, y_val = train_x[val_idx], train_y[val_idx]\n",
        "      training_generator = MixupGenerator(x_train, y_train, batch_size=128,datagen=imagegen)()\n",
        "      checkpoint = ModelCheckpoint(main_path+person+f'/check{e}.h5',save_best_only=True)\n",
        "      model = build_fn()\n",
        "      model.compile(optimizer = tf.keras.optimizers.Adam(0.002),\n",
        "                  loss = keras.losses.CategoricalCrossentropy(),\n",
        "                  )\n",
        "\n",
        "      history = model.fit(training_generator, steps_per_epoch=x_train.shape[0] //128, validation_data = (x_val, y_val), epochs = 30, callbacks=[early_stopping, checkpoint])\n",
        "      gc.collect()\n",
        "      print(\"fold :\", e)\n",
        "    print(\"*******************************************************************\")"
      ],
      "metadata": {
        "id": "IKewyodrSjqU"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## 테스트"
      ],
      "metadata": {
        "id": "_VxDq5TTq4FG"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## 예측 1(Minmax)"
      ],
      "metadata": {
        "id": "cutx4HGCq9rY"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "test_x= np.load(main_path+'/test_m.npy')"
      ],
      "metadata": {
        "id": "mCH48Hp9rVQz"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "pred2 = []\n",
        "pred2_ = []\n",
        "early_stopping = EarlyStopping(patience=10)\n",
        "\n",
        "for e in range(10):\n",
        "    model = build_fn()\n",
        "    model.load_weights(main_path+f'/checkcheck{e}.h5')\n",
        "    print(\"*******************************************************************\")\n",
        "    pred2.append(model.predict(test_x))\n",
        "    pred2_.append(np.argmax(model.predict(test_x), axis = 1))\n",
        "    print(\"*******************************************************************\")"
      ],
      "metadata": {
        "id": "QLb3DztvrVTZ"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## 예측 2(std)"
      ],
      "metadata": {
        "id": "aw12Ii6WrFav"
      }
    },
    {
      "cell_type": "markdown",
      "source": [],
      "metadata": {
        "id": "XkV6hd40rB3K"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "test_x= np.load(main_path+'/test_s.npy')"
      ],
      "metadata": {
        "id": "pDlB5V9Kq29B"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "pred1 = []\n",
        "pred1_ = []\n",
        "early_stopping = EarlyStopping(patience=10)\n",
        "\n",
        "for e in range(5):\n",
        "    model = build_fn()\n",
        "    model.load_weights(main_path+f'/check{e}.h5')\n",
        "    print(\"*******************************************************************\")\n",
        "    pred1.append(model.predict(test_x))\n",
        "    pred1_.append(np.argmax(model.predict(test_x), axis = 1))\n",
        "    print(\"*******************************************************************\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "rXxY2R6IrUR4",
        "outputId": "5595691f-3b4b-4bc1-f2a2-aad8b4a2e1e5"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "*******************************************************************\n",
            "191/191 [==============================] - 10s 49ms/step\n",
            "191/191 [==============================] - 9s 49ms/step\n",
            "*******************************************************************\n",
            "*******************************************************************\n",
            "191/191 [==============================] - 10s 49ms/step\n",
            "191/191 [==============================] - 9s 49ms/step\n",
            "*******************************************************************\n",
            "*******************************************************************\n",
            "191/191 [==============================] - 10s 49ms/step\n",
            "191/191 [==============================] - 9s 49ms/step\n",
            "*******************************************************************\n",
            "*******************************************************************\n",
            "191/191 [==============================] - 10s 48ms/step\n",
            "191/191 [==============================] - 9s 48ms/step\n",
            "*******************************************************************\n",
            "*******************************************************************\n",
            "191/191 [==============================] - 10s 49ms/step\n",
            "191/191 [==============================] - 9s 49ms/step\n",
            "*******************************************************************\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## 6100"
      ],
      "metadata": {
        "id": "xrmiuiJArq2J"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "test_ = pd.DataFrame(index = range(0, 6100), columns = [\"path\", \"id\"])\n",
        "test_[\"path\"] = [main_path + f\"/open/test/{i+1}.wav\" for i in range(6100)]\n",
        "test_[\"id\"] =[i+1 for i in range(6100)]\n",
        "test_.head()"
      ],
      "metadata": {
        "id": "AaQQBTYQrZ7w"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "sample_submission = pd.read_csv(main_path + \"/open/sample_submission.csv\")"
      ],
      "metadata": {
        "id": "61rFSO5wrbSr"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "result1 = pd.concat([test_, pd.DataFrame(np.mean(pred1, axis = 0))], axis = 1).iloc[:, 1:]\n",
        "result1 = pd.merge(sample_submission[\"id\"], result1)\n",
        "result1.columns = sample_submission.columns"
      ],
      "metadata": {
        "id": "CNUesNPWrccM"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "result2 = pd.concat([test_, pd.DataFrame(np.mean(pred2, axis = 0))], axis = 1).iloc[:, 1:]\n",
        "result2 = pd.merge(sample_submission[\"id\"], result2)\n",
        "result2.columns = sample_submission.columns"
      ],
      "metadata": {
        "id": "XlkhG8PSrdfp"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "result=result1.copy()\n",
        "result.iloc[:,1:]=(result1.iloc[:,1:]+result2.iloc[:,1:])/2\n",
        "result"
      ],
      "metadata": {
        "id": "rdyskqVjre5p"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "result.to_csv(main_path+\"/audio.csv\", index = False)"
      ],
      "metadata": {
        "id": "962xtTNZrfsC"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}